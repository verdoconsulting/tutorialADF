{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"factoryName": {
			"type": "string",
			"metadata": "Data Factory name",
			"defaultValue": "df-tutorialadf"
		}
	},
	"variables": {
		"factoryId": "[concat('Microsoft.DataFactory/factories/', parameters('factoryName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('factoryName'), '/DF_FactLoader')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "This is a data flow example of how to load facts into your fact table from a single sample Employee dimension. There is also an example of handling early-arriving facts.",
				"folder": {
					"name": "Templates"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Facts",
								"type": "DatasetReference"
							},
							"name": "Facts"
						},
						{
							"dataset": {
								"referenceName": "DimEmp1",
								"type": "DatasetReference"
							},
							"name": "DimEmployee"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "FactTable2",
								"type": "DatasetReference"
							},
							"name": "writeFactTable"
						}
					],
					"transformations": [
						{
							"name": "SetAttributes"
						},
						{
							"name": "LookupDimEmployee"
						},
						{
							"name": "DailyAggs"
						},
						{
							"name": "OrigData"
						},
						{
							"name": "JoinAllColumns"
						},
						{
							"name": "CheckForEarlyFact"
						}
					],
					"script": "source(output(\n\t\temployeeID as string,\n\t\tHours as string,\n\t\tAmount as string,\n\t\tProjDate as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\twildcardPaths:['SampleData/Facts/today/*.csv']) ~> Facts\nsource(output(\n\t\tEmpID as integer,\n\t\tsurrogatekey as string,\n\t\tRegion as string,\n\t\tStatus as string,\n\t\tEmpFunction as string,\n\t\tLevel as string,\n\t\tRole as string,\n\t\tStartDate as date,\n\t\tEndDate as date,\n\t\tiscurrent as integer,\n\t\tprocesstime as timestamp\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tisolationLevel: 'READ_UNCOMMITTED',\n\tformat: 'table') ~> DimEmployee\nFacts derive(employeeID = toInteger(employeeID),\n\t\tiscurrent = 1,\n\t\tfacttimestamp = currentTimestamp()) ~> SetAttributes\nSetAttributes, DimEmployee lookup(employeeID == EmpID\n\t&& SetAttributes@iscurrent == DimEmployee@iscurrent,\n\tmultiple: true,\n\tbroadcast: 'auto')~> LookupDimEmployee\nCheckForEarlyFact aggregate(groupBy(ProjDate,\n\t\temployeeID),\n\tTotalHours = sum(toInteger(Hours)),\n\t\tTotalAmount = sum(toInteger(Amount))) ~> DailyAggs\nLookupDimEmployee select(mapColumn(\n\t\temployeeID,\n\t\tHours,\n\t\tAmount,\n\t\tProjDate,\n\t\tfacttimestamp,\n\t\tEmpID,\n\t\tsurrogatekey,\n\t\tRegion,\n\t\tStatus,\n\t\tEmpFunction,\n\t\tLevel,\n\t\tRole,\n\t\tStartDate,\n\t\tEndDate,\n\t\tiscurrent = DimEmployee@iscurrent,\n\t\tprocesstime\n\t),\n\tskipDuplicateMapInputs: false,\n\tskipDuplicateMapOutputs: false) ~> OrigData\nDailyAggs, OrigData join(DailyAggs@ProjDate == OrigData@ProjDate\n\t&& DailyAggs@employeeID == OrigData@employeeID,\n\tjoinType:'inner',\n\tbroadcast: 'auto')~> JoinAllColumns\nLookupDimEmployee derive(employeeID = iif(isNull(employeeID),0,employeeID)) ~> CheckForEarlyFact\nJoinAllColumns sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tdeletable:false,\n\tinsertable:true,\n\tupdateable:false,\n\tupsertable:false,\n\tformat: 'table',\n\tbatchSize: 50,\n\tmapColumn(\n\t\tProjDate = OrigData@ProjDate,\n\t\temployeeID = OrigData@employeeID,\n\t\tTotalHours,\n\t\tTotalAmount,\n\t\tfacttimestamp,\n\t\tsurrogatekey,\n\t\tRegion,\n\t\tStatus,\n\t\tEmpFunction,\n\t\tLevel,\n\t\tRole,\n\t\tprocesstime\n\t),\n\tpartitionBy('roundRobin', 4)) ~> writeFactTable"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/DimEmployeeLoader2')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"folder": {
					"name": "Templates"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "EmployeeFiles",
								"type": "DatasetReference"
							},
							"name": "Employees1",
							"description": " Source employees file, changes every day"
						},
						{
							"dataset": {
								"referenceName": "DimEmp",
								"type": "DatasetReference"
							},
							"name": "DimEmployees",
							"description": "Current rows in DimEmployees DW table"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "DimEmp",
								"type": "DatasetReference"
							},
							"name": "sinkNew",
							"description": " "
						},
						{
							"dataset": {
								"referenceName": "DimEmp",
								"type": "DatasetReference"
							},
							"name": "sinkUpdates",
							"description": " "
						},
						{
							"dataset": {
								"referenceName": "DimEmp",
								"type": "DatasetReference"
							},
							"name": "sinkInactive",
							"description": " Age out old rows"
						}
					],
					"transformations": [
						{
							"name": "TypeConversions"
						},
						{
							"name": "TypeConversionsAndSetAttrs"
						},
						{
							"name": "LookupIDs"
						},
						{
							"name": "ConditionalSplit1"
						},
						{
							"name": "checkForChanges"
						},
						{
							"name": "SetAttrsForNew"
						},
						{
							"name": "SetAttrsInactive",
							"description": "make iscurrent 0"
						},
						{
							"name": "SetAttrsUpdate"
						},
						{
							"name": "NormNames"
						},
						{
							"name": "InactiveFields"
						},
						{
							"name": "AlterRow1"
						},
						{
							"name": "NullFilter",
							"description": "Filter out NULLs from source file"
						},
						{
							"name": "NameNorm2"
						}
					],
					"script": "source(output(\n\t\tEmpID as string,\n\t\tRegion as string,\n\t\tStatus as string,\n\t\tFunction as string,\n\t\tLevel as string,\n\t\tRole as string,\n\t\tStartDate as string,\n\t\tEndDate as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tpurgeFiles: true,\n\twildcardPaths:['SampleData/Emps/today/*.csv']) ~> Employees1\nsource(output(\n\t\tEmpID as integer,\n\t\tsurrogatekey as string,\n\t\tRegion as string,\n\t\tStatus as string,\n\t\tEmpFunction as string,\n\t\tLevel as string,\n\t\tRole as string,\n\t\tStartDate as date,\n\t\tEndDate as date,\n\t\tiscurrent as integer,\n\t\tprocesstime as timestamp\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tisolationLevel: 'READ_UNCOMMITTED',\n\tformat: 'table') ~> DimEmployees\nDimEmployees derive(EmpID = toInteger(EmpID)) ~> TypeConversions\nNullFilter derive(EmpID = toInteger(EmpID),\n\t\tStartDate = toDate(StartDate,'MM/dd/yyyy'),\n\t\tEndDate = toDate(EndDate,'MM/dd/yyyy'),\n\t\tprocesstime = currentTimestamp()) ~> TypeConversionsAndSetAttrs\nTypeConversionsAndSetAttrs, TypeConversions lookup(TypeConversionsAndSetAttrs@EmpID == TypeConversions@EmpID,\n\tmultiple: true,\n\tbroadcast: 'auto')~> LookupIDs\nNormNames split(isNull(iscurrent),\n\tdisjoint: false) ~> ConditionalSplit1@(NewRow, CheckForUpdates)\nNameNorm2, TypeConversions exists(NameNorm2@EmpID == TypeConversions@EmpID,\n\tnegate:false,\n\tbroadcast: 'auto')~> checkForChanges\nConditionalSplit1@NewRow derive(iscurrent = 1,\n\t\tsurrogatekey = toString(crc32(EmpID,EmpFunction))) ~> SetAttrsForNew\ncheckForChanges derive(iscurrent = 0) ~> SetAttrsInactive\ncheckForChanges derive(iscurrent = 1) ~> SetAttrsUpdate\nLookupIDs select(mapColumn(\n\t\tEmpID = TypeConversionsAndSetAttrs@EmpID,\n\t\tRegion = Employees1@Region,\n\t\tStatus = Employees1@Status,\n\t\tLevel = Employees1@Level,\n\t\tRole = Employees1@Role,\n\t\tStartDate = TypeConversionsAndSetAttrs@StartDate,\n\t\tEndDate = TypeConversionsAndSetAttrs@EndDate,\n\t\tEmpFunction = Function,\n\t\tiscurrent,\n\t\tprocesstime = TypeConversionsAndSetAttrs@processtime,\n\t\tsurrogatekey\n\t),\n\tskipDuplicateMapInputs: false,\n\tskipDuplicateMapOutputs: false) ~> NormNames\nSetAttrsInactive select(mapColumn(\n\t\tEmpID,\n\t\tStatus,\n\t\tEndDate,\n\t\tiscurrent,\n\t\tprocesstime\n\t),\n\tskipDuplicateMapInputs: false,\n\tskipDuplicateMapOutputs: false) ~> InactiveFields\nInactiveFields alterRow(updateIf(true())) ~> AlterRow1\nEmployees1 filter(!isNull(EmpID)) ~> NullFilter\nConditionalSplit1@CheckForUpdates select(mapColumn(\n\t\tEmpID,\n\t\tRegion,\n\t\tStatus,\n\t\tLevel,\n\t\tRole,\n\t\tStartDate,\n\t\tEndDate,\n\t\tEmpFunction,\n\t\tiscurrent,\n\t\tprocesstime,\n\t\tsurrogatekey\n\t),\n\tskipDuplicateMapInputs: false,\n\tskipDuplicateMapOutputs: false) ~> NameNorm2\nSetAttrsForNew sink(input(\n\t\tEmpID as integer,\n\t\tsurrogatekey as string,\n\t\tRegion as string,\n\t\tStatus as string,\n\t\tEmpFunction as string,\n\t\tLevel as string,\n\t\tRole as string,\n\t\tStartDate as date,\n\t\tEndDate as date,\n\t\tiscurrent as integer,\n\t\tprocesstime as timestamp\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tdeletable:false,\n\tinsertable:true,\n\tupdateable:false,\n\tupsertable:false,\n\tformat: 'table',\n\tbatchSize: 50,\n\tpartitionBy('roundRobin', 4)) ~> sinkNew\nSetAttrsUpdate sink(input(\n\t\tEmpID as integer,\n\t\tsurrogatekey as string,\n\t\tRegion as string,\n\t\tStatus as string,\n\t\tEmpFunction as string,\n\t\tLevel as string,\n\t\tRole as string,\n\t\tStartDate as date,\n\t\tEndDate as date,\n\t\tiscurrent as integer,\n\t\tprocesstime as timestamp\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tdeletable:false,\n\tinsertable:true,\n\tupdateable:false,\n\tupsertable:false,\n\tformat: 'table',\n\tbatchSize: 50,\n\tpartitionBy('roundRobin', 4)) ~> sinkUpdates\nAlterRow1 sink(input(\n\t\tEmpID as integer,\n\t\tsurrogatekey as string,\n\t\tRegion as string,\n\t\tStatus as string,\n\t\tEmpFunction as string,\n\t\tLevel as string,\n\t\tRole as string,\n\t\tStartDate as date,\n\t\tEndDate as date,\n\t\tiscurrent as integer,\n\t\tprocesstime as timestamp\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tdeletable:false,\n\tinsertable:false,\n\tupdateable:true,\n\tupsertable:false,\n\tkeys:['EmpID'],\n\tformat: 'table',\n\tbatchSize: 50,\n\tmapColumn(\n\t\tEmpID,\n\t\tEndDate,\n\t\tiscurrent,\n\t\tprocesstime\n\t),\n\tpartitionBy('roundRobin', 4)) ~> sinkInactive"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/GenericSCDType2')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"folder": {
					"name": "Templates"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "GenericDataset",
								"type": "DatasetReference"
							},
							"name": "GenericInput"
						},
						{
							"dataset": {
								"referenceName": "SqlDimension",
								"type": "DatasetReference"
							},
							"name": "ExistingDimensionTable"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "SqlDimension",
								"type": "DatasetReference"
							},
							"name": "DimensionTableSink"
						}
					],
					"transformations": [
						{
							"name": "NewAndUpdatedRows"
						},
						{
							"name": "AddHashInput"
						},
						{
							"name": "AddHashExisting"
						},
						{
							"name": "GetMaxSurrogateKey"
						},
						{
							"name": "AddKey"
						},
						{
							"name": "JoinWithMaxSurrogateKey"
						},
						{
							"name": "AddDimensionColumns"
						},
						{
							"name": "FilterForUpdatedValues"
						},
						{
							"name": "UpdateObsolete"
						},
						{
							"name": "DropUnwantedColsInput"
						},
						{
							"name": "UnionAllData"
						},
						{
							"name": "MarkAsUpdate"
						},
						{
							"name": "DropUnwantedColumns"
						},
						{
							"name": "MarkAsInsert"
						},
						{
							"name": "FilterForActive"
						}
					],
					"script": "parameters{\n\tPrimaryKey as string ('ID'),\n\tColumns as string ('Player,Team,Salary')\n}\nsource(allowSchemaDrift: true,\n\tvalidateSchema: false) ~> GenericInput\nsource(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tisolationLevel: 'READ_UNCOMMITTED',\n\tformat: 'table') ~> ExistingDimensionTable\nAddHashInput, AddHashExisting exists(AddHashInput@id_hash == AddHashExisting@id_hash\n\t&& AddHashInput@columns_hash == AddHashExisting@columns_hash,\n\tnegate:true,\n\tbroadcast: 'auto')~> NewAndUpdatedRows\nGenericInput derive(id_hash = md5(byName($PrimaryKey)),\n\t\tcolumns_hash = md5(byNames(split($Columns,',')))) ~> AddHashInput\nFilterForActive derive(id_hash = md5(byNames(split($PrimaryKey,','))),\n\t\tcolumns_hash = md5(byNames(split($Columns,',')))) ~> AddHashExisting\nAddHashExisting aggregate(MaxSurrogateKey = max(toInteger(byName('Key')))) ~> GetMaxSurrogateKey\nNewAndUpdatedRows keyGenerate(output(Key as long),\n\tstartAt: 1L) ~> AddKey\nAddKey, GetMaxSurrogateKey join(Key == MaxSurrogateKey || true(),\n\tjoinType:'cross',\n\tbroadcast: 'right')~> JoinWithMaxSurrogateKey\nJoinWithMaxSurrogateKey derive(Key = Key + MaxSurrogateKey,\n\t\tActive = 1,\n\t\tActiveStartTime = currentUTC(),\n\t\tActiveEndTime = toTimestamp(toString(null()))) ~> AddDimensionColumns\nAddHashExisting, NewAndUpdatedRows exists(AddHashExisting@id_hash == AddHashInput@id_hash,\n\tnegate:false,\n\tbroadcast: 'auto')~> FilterForUpdatedValues\nFilterForUpdatedValues derive(Active = 0,\n\t\tActiveEndTime = currentUTC()) ~> UpdateObsolete\nAddDimensionColumns select(mapColumn(\n\t\teach(match(!in(['id_hash','columns_hash','MaxSurrogateKey'],name)))\n\t),\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~> DropUnwantedColsInput\nMarkAsInsert, DropUnwantedColumns union(byName: true)~> UnionAllData\nUpdateObsolete alterRow(updateIf(true())) ~> MarkAsUpdate\nMarkAsUpdate select(mapColumn(\n\t\teach(match(!in(['id_hash','columns_hash','MaxSurrogateKey'],name)))\n\t),\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~> DropUnwantedColumns\nDropUnwantedColsInput alterRow(insertIf(true())) ~> MarkAsInsert\nExistingDimensionTable filter(toInteger(byName('Active')) == 1) ~> FilterForActive\nUnionAllData sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tdeletable:false,\n\tinsertable:true,\n\tupdateable:true,\n\tupsertable:false,\n\tkeys:[($PrimaryKey)],\n\tformat: 'table',\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~> DimensionTableSink"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_AgregarVentasMes')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "DF Para tomar los datos de ventas, agregarlos e insertarlos ordenados por ANNOMES que es el Order Date",
				"folder": {
					"name": "Ejemplos Ventas"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "datosVentas",
								"type": "DatasetReference"
							},
							"name": "datosVentas",
							"description": "Origen con 1,5 millones de registros de Ventas"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "datosVentasMensuales",
								"type": "DatasetReference"
							},
							"name": "datosVentasMensuales",
							"description": "Target de Ventas Mensuales por Order Date"
						}
					],
					"transformations": [
						{
							"name": "totalizarMedidas",
							"description": "Totalización por Order Date en formato numérico"
						},
						{
							"name": "ordenarAAAAMM",
							"description": "Se ordena el Order Date pero en formato numérico YYYYMM"
						},
						{
							"name": "inclusionFechaNumerica",
							"description": "Cambio a formato numérico de \"Order Date\""
						}
					],
					"script": "source(output(\n\t\tRegion as string,\n\t\tCountry as string,\n\t\t{Item Type} as string,\n\t\t{Sales Channel} as string,\n\t\t{Order Priority} as string,\n\t\t{Order Date} as date,\n\t\t{Order ID} as string,\n\t\t{Ship Date} as date,\n\t\t{Units Sold} as decimal(18,2),\n\t\t{Unit Price} as decimal(18,2),\n\t\t{Unit Cost} as decimal(18,2),\n\t\t{Total Revenue} as decimal(18,2),\n\t\t{Total Cost} as decimal(18,2),\n\t\t{Total Profit} as decimal(18,2)\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tisolationLevel: 'READ_UNCOMMITTED',\n\tformat: 'table') ~> datosVentas\nordenarAAAAMM aggregate(groupBy(ANNOMES = {Order Date}),\n\t{Units Sold} = sum({Units Sold}),\n\t\t{Unit Price} = avg({Unit Price}),\n\t\t{Unit Cost} = avg({Unit Cost}),\n\t\t{Total Revenue} = avg({Total Revenue}),\n\t\t{Total Cost} = avg({Total Cost}),\n\t\t{Total Profit} = avg({Total Profit}),\n\tpartitionBy('hash', 1)) ~> totalizarMedidas\ninclusionFechaNumerica sort(asc({Order Date}, true),\n\tcaseInsensitive: true,\n\tpartitionLevel: true) ~> ordenarAAAAMM\ndatosVentas derive({Order Date} = toInteger(toString({Order Date},'YYYYMM'))) ~> inclusionFechaNumerica\ntotalizarMedidas sink(input(\n\t\tANNOMES as integer,\n\t\t{Units Sold} as decimal(28,2),\n\t\t{Unit Price} as decimal(22,6),\n\t\t{Unit Cost} as decimal(22,6),\n\t\t{Total Revenue} as decimal(22,6),\n\t\t{Total Cost} as decimal(22,6),\n\t\t{Total Profit} as decimal(22,6)\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false,\n\tdeletable:false,\n\tinsertable:true,\n\tupdateable:false,\n\tupsertable:false,\n\ttruncate:true,\n\tformat: 'table',\n\tmapColumn(\n\t\tANNOMES,\n\t\t{Units Sold},\n\t\t{Unit Price},\n\t\t{Unit Cost},\n\t\t{Total Revenue},\n\t\t{Total Cost},\n\t\t{Total Profit}\n\t),\n\tpartitionBy('hash', 1),\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~> datosVentasMensuales"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_TransformMovies')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"folder": {
					"name": "Ejemplos Películas"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "ds_moviesDB",
								"type": "DatasetReference"
							},
							"name": "MoviesDB",
							"description": "Fichero origen de películas"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "sink_MoviesDB",
								"type": "DatasetReference"
							},
							"name": "MovieSink"
						}
					],
					"transformations": [
						{
							"name": "FilterYears",
							"description": "Filtrado de años"
						},
						{
							"name": "AggregateComedyRatings",
							"description": "Agregación de ratings por año"
						},
						{
							"name": "OrdenarAnos",
							"description": "Ordenación en base al año numérico"
						},
						{
							"name": "LeerPeliculas",
							"description": "Leemos nuevamente las películas para enlazar al set de datos completo."
						},
						{
							"name": "EnlacePeliculas",
							"description": "Self-join para tener todas las columnas"
						},
						{
							"name": "SelColumnas",
							"description": "Se elimina una de las columnas año"
						}
					],
					"script": "source(output(\n\t\tmovie as string,\n\t\ttitle as string,\n\t\tgenres as string,\n\t\tyear as string,\n\t\tRating as string,\n\t\t{Rotton Tomato} as string\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false) ~> MoviesDB\nMoviesDB filter(toInteger(year) >= 1910 && toInteger(year) <= 2000 && rlike(genres, 'Comedy'),\n\tpartitionBy('hash', 1)) ~> FilterYears\nFilterYears aggregate(groupBy(year),\n\tAverageComedyRating = round(avg(toInteger(Rating)),3)) ~> AggregateComedyRatings\nSelColumnas sort(asc(toInteger(anno), true),\n\tcaseInsensitive: true,\n\tpartitionLevel: true) ~> OrdenarAnos\nMoviesDB select(mapColumn(\n\t\tmovie,\n\t\ttitle,\n\t\tgenres,\n\t\tyear_original = year,\n\t\tRating,\n\t\t{Rotton Tomato}\n\t),\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~> LeerPeliculas\nAggregateComedyRatings, LeerPeliculas join(year == year_original,\n\tjoinType:'inner',\n\tbroadcast: 'auto')~> EnlacePeliculas\nEnlacePeliculas select(mapColumn(\n\t\tid_pelicula = movie,\n\t\ttitulo = title,\n\t\tanno = year_original,\n\t\tgenero = genres,\n\t\tRating,\n\t\tPromedioRatingAnual = AverageComedyRating,\n\t\t{Rotton Tomato}\n\t),\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~> SelColumnas\nOrdenarAnos sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tpartitionFileNames:['PeliculasOrdenadas_20200623.csv'],\n\tpartitionBy('hash', 1),\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~> MovieSink"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_smallRadio')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"folder": {
					"name": "Ejemplos Radio"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "smRadio",
								"type": "DatasetReference"
							},
							"name": "smallRadio",
							"description": "Origen JSON"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "ds_datosRadio",
								"type": "DatasetReference"
							},
							"name": "datosRadios"
						}
					],
					"transformations": [
						{
							"name": "seleccionarColumnas",
							"description": "Selección de algunas columnas solamente"
						},
						{
							"name": "SubscribersByLocation"
						},
						{
							"name": "countOfSubTypeByGender"
						},
						{
							"name": "datosOriginales"
						},
						{
							"name": "subscribersByLocDatosOri"
						},
						{
							"name": "countOfSubTypeByGenderDatosOri"
						},
						{
							"name": "LimpiarColumnas"
						},
						{
							"name": "columnasAdicionales"
						}
					],
					"script": "source(output(\n\t\tts as string,\n\t\tuserId as string,\n\t\tsessionId as string,\n\t\tpage as string,\n\t\tauth as string,\n\t\tmethod as string,\n\t\tstatus as string,\n\t\tlevel as string,\n\t\titemInSession as string,\n\t\tlocation as string,\n\t\tlastName as string,\n\t\tfirstName as string,\n\t\tregistration as string,\n\t\tgender as string,\n\t\tartist as string,\n\t\tsong as string,\n\t\tlength as double\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false) ~> smallRadio\nsmallRadio select(mapColumn(\n\t\ttime_stamp = ts,\n\t\tpage,\n\t\tmethod,\n\t\tstatus,\n\t\tlevel_type = level,\n\t\tlocation,\n\t\tlastName,\n\t\tfirstName,\n\t\tregistration,\n\t\tgender\n\t),\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~> seleccionarColumnas\nseleccionarColumnas aggregate(groupBy(location),\n\tSubscribersByLocation = count(),\n\t\ttime_stamp = first(time_stamp)) ~> SubscribersByLocation\nseleccionarColumnas aggregate(groupBy(gender,\n\t\tlevel_type),\n\tcountOfSubTypeByGender = count(),\n\t\ttime_stamp = first(time_stamp)) ~> countOfSubTypeByGender\nseleccionarColumnas select(mapColumn(\n\t\ttime_stamp,\n\t\tpage,\n\t\tmethod,\n\t\tstatus,\n\t\tlevel_type,\n\t\tlocation,\n\t\tlastName,\n\t\tfirstName,\n\t\tregistration,\n\t\tgender\n\t),\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~> datosOriginales\ndatosOriginales, SubscribersByLocation join(datosOriginales@time_stamp == SubscribersByLocation@time_stamp,\n\tjoinType:'left',\n\tbroadcast: 'auto')~> subscribersByLocDatosOri\nsubscribersByLocDatosOri, countOfSubTypeByGender join(datosOriginales@time_stamp == countOfSubTypeByGender@time_stamp,\n\tjoinType:'left',\n\tbroadcast: 'auto')~> countOfSubTypeByGenderDatosOri\ncountOfSubTypeByGenderDatosOri select(mapColumn(\n\t\ttime_stamp = datosOriginales@time_stamp,\n\t\tstatus,\n\t\tlevel_type = datosOriginales@level_type,\n\t\tlocation = datosOriginales@location,\n\t\tlastName,\n\t\tfirstName,\n\t\tregistration,\n\t\tgender = datosOriginales@gender,\n\t\tlocation = SubscribersByLocation@location,\n\t\tSubscribersByLocation,\n\t\ttime_stamp = SubscribersByLocation@time_stamp,\n\t\tgender = countOfSubTypeByGender@gender,\n\t\tlevel_type = countOfSubTypeByGender@level_type,\n\t\tcountOfSubTypeByGender,\n\t\ttime_stamp = countOfSubTypeByGender@time_stamp\n\t),\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~> LimpiarColumnas\nLimpiarColumnas derive(nombre_completo = firstName + ' ' + lastName,\n\t\testado = split(location, ',')[2]) ~> columnasAdicionales\ncolumnasAdicionales sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tdeletable:false,\n\tinsertable:true,\n\tupdateable:false,\n\tupsertable:false,\n\trecreate:true,\n\tformat: 'table',\n\tmapColumn(\n\t\ttime_stamp,\n\t\tnombre_completo,\n\t\tstatus,\n\t\tlevel_type,\n\t\tlocation,\n\t\tregistration,\n\t\tgender,\n\t\testado,\n\t\tSubscribersByLocation,\n\t\tcountOfSubTypeByGender\n\t),\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~> datosRadios"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/df_detalleRadio')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"folder": {
					"name": "Ejemplos Radio"
				},
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "smRadio",
								"type": "DatasetReference"
							},
							"name": "smallRadio"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "ds_detalleRadio",
								"type": "DatasetReference"
							},
							"name": "datosDetallados"
						}
					],
					"transformations": [],
					"script": "source(output(\n\t\tts as string,\n\t\tuserId as string,\n\t\tsessionId as string,\n\t\tpage as string,\n\t\tauth as string,\n\t\tmethod as string,\n\t\tstatus as string,\n\t\tlevel as string,\n\t\titemInSession as string,\n\t\tlocation as string,\n\t\tlastName as string,\n\t\tfirstName as string,\n\t\tregistration as string,\n\t\tgender as string,\n\t\tartist as string,\n\t\tsong as string,\n\t\tlength as double\n\t),\n\tallowSchemaDrift: true,\n\tvalidateSchema: false) ~> smallRadio\nsmallRadio sink(allowSchemaDrift: true,\n\tvalidateSchema: false,\n\tdeletable:false,\n\tinsertable:true,\n\tupdateable:false,\n\tupsertable:false,\n\trecreate:true,\n\tformat: 'table',\n\tskipDuplicateMapInputs: true,\n\tskipDuplicateMapOutputs: true) ~> datosDetallados"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/DimEmployeePipeline')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "LoadDimEmployee",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 3,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DimEmployeeLoader2",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"Employees1": {},
									"DimEmployees": {},
									"sinkNew": {},
									"sinkUpdates": {},
									"sinkInactive": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							}
						}
					}
				],
				"folder": {
					"name": "Templates"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/DimEmployeeLoader2')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/GenericSCDType2')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"description": "Using byNames to fully parameterize a Slowly Change Dimension Type 2 patter",
				"activities": [
					{
						"name": "GenericSCDType2",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "GenericSCDType2",
								"type": "DataFlowReference",
								"parameters": {
									"PrimaryKey": {
										"value": "'@{pipeline().parameters.PrimaryKey}'",
										"type": "Expression"
									},
									"Columns": {
										"value": "'@{pipeline().parameters.ColumnNames}'",
										"type": "Expression"
									}
								},
								"datasetParameters": {
									"GenericInput": {
										"Folder": {
											"value": "@pipeline().parameters.IncomingDimensionFolder",
											"type": "Expression"
										}
									},
									"ExistingDimensionTable": {
										"Table": {
											"value": "@pipeline().parameters.DimensionTable",
											"type": "Expression"
										}
									},
									"DimensionTableSink": {
										"Table": {
											"value": "@pipeline().parameters.DimensionTable",
											"type": "Expression"
										}
									}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							}
						}
					}
				],
				"parameters": {
					"IncomingDimensionFolder": {
						"type": "string"
					},
					"DimensionTable": {
						"type": "string"
					},
					"PrimaryKey": {
						"type": "string"
					},
					"ColumnNames": {
						"type": "string"
					}
				},
				"folder": {
					"name": "Templates"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/GenericSCDType2')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/LoadFacts2')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Data Flow Fact Loader",
						"description": "This is a data flow example of how to load facts into your fact table from a single sample Employee dimension. There is also an example of handling early-arriving facts.",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "DF_FactLoader",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"Facts": {},
									"DimEmployee": {},
									"writeFactTable": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							}
						}
					}
				],
				"folder": {
					"name": "Templates"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/DF_FactLoader')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/TransformMovies')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "TransformarPeliculas",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "df_TransformMovies",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"MoviesDB": {},
									"MovieSink": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							}
						}
					}
				],
				"folder": {
					"name": "Ejemplos Películas"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/df_TransformMovies')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/pl_df_AgregarVentasMes')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "df_AgregarVentasMes",
						"description": "No he logrado hacer que los datos se ordenen al ser insertados en el Sink.",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "df_AgregarVentasMes",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"datosVentas": {},
									"datosVentasMensuales": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							}
						}
					}
				],
				"folder": {
					"name": "Ejemplos Ventas"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/df_AgregarVentasMes')]"
			]
		},
		{
			"name": "[concat(parameters('factoryName'), '/pl_df_Radio')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "df_smallRadio",
						"type": "ExecuteDataFlow",
						"dependsOn": [
							{
								"activity": "df_detalleRadio",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "df_smallRadio",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"smallRadio": {},
									"datosRadios": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							}
						}
					},
					{
						"name": "df_detalleRadio",
						"description": "DF que carga el JSON sin ningún cambio.",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "7.00:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "df_detalleRadio",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"smallRadio": {},
									"datosDetallados": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							}
						}
					}
				],
				"folder": {
					"name": "Ejemplos Radio"
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/df_smallRadio')]",
				"[concat(variables('factoryId'), '/dataflows/df_detalleRadio')]"
			]
		}
	]
}